{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be1b95e",
   "metadata": {},
   "source": [
    "<h2 style=\"color: blue;\">Project: NLP Assignment</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416bc0e",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e68d37b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing text for URL_ID: blackassign0036\n",
      "Missing text for URL_ID: blackassign0049\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read input data\n",
    "input_data = pd.read_excel(\"Input.xlsx\")\n",
    "\n",
    "# Function to extract article title and text from URL\n",
    "def extract_title_and_text(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract article title\n",
    "    title = soup.title.string if soup.title else \"\"\n",
    "\n",
    "    # Potential classes for the content div\n",
    "    potential_classes = ['td-post-content tagdiv-type', 'td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type']  # Add more classes as needed\n",
    "\n",
    "    # Attempt to find the content div using different classes\n",
    "    article_text = \"\"\n",
    "    for class_name in potential_classes:\n",
    "        article_body = soup.find('div', class_=class_name)\n",
    "        if article_body:\n",
    "            article_text = article_body.get_text()\n",
    "            break  # Stop searching once content is found\n",
    "\n",
    "    return title, article_text\n",
    "\n",
    "# Loop through each row in the input data\n",
    "for index, row in input_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Extract article title and text\n",
    "    title, article_text = extract_title_and_text(url)\n",
    "\n",
    "    # Save extracted title and text to a file with URL_ID as the file name\n",
    "    with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"Title: {title}\\n\\nArticle Text: {article_text}\")\n",
    "\n",
    "    # Print a message if no content is found\n",
    "    if not article_text.strip():\n",
    "        print(f\"Missing text for URL_ID: {url_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f354f",
   "metadata": {},
   "source": [
    "#### Loading the Stopwords and MasterDictionary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98aab5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Specify the path to the folders\n",
    "stop_words_folder = 'StopWords'\n",
    "master_dict_folder = 'MasterDictionary'\n",
    "\n",
    "# Load Stop Words Lists\n",
    "stop_words_files = os.listdir(stop_words_folder)\n",
    "stop_words_set = set()\n",
    "\n",
    "for stop_words_file in stop_words_files:\n",
    "    with open(os.path.join(stop_words_folder, stop_words_file), 'r', encoding='ISO-8859-1') as file:\n",
    "        stop_words_set.update(file.read().splitlines())\n",
    "\n",
    "# Load Positive and Negative words from Master Dictionary\n",
    "positive_words_path = os.path.join(master_dict_folder, 'positive-words.txt')\n",
    "negative_words_path = os.path.join(master_dict_folder, 'negative-words.txt')\n",
    "\n",
    "positive_words = set(open(positive_words_path, 'r', encoding='ISO-8859-1').read().splitlines())\n",
    "negative_words = set(open(negative_words_path, 'r', encoding='ISO-8859-1').read().splitlines())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce2c4dc",
   "metadata": {},
   "source": [
    "#### Preparing analysis function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ad378ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = text.split('.')  # Assuming that '.' is used to denote the end of a sentence\n",
    "    words = text.split()  # Splitting by space to get individual words\n",
    "\n",
    "\n",
    "    # Remove stop words\n",
    "    cleaned_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words_set]\n",
    "\n",
    "    # Calculate Positive Score\n",
    "    positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "\n",
    "    # Calculate Negative Score\n",
    "    negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "\n",
    "    # Calculate Polarity Score\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "    # Calculate Subjectivity Score\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n",
    "\n",
    "    # Calculate Average Sentence Length\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "\n",
    "    # Calculate Percentage of Complex Words\n",
    "    complex_words = [word for word in cleaned_words if len(word) > 2]\n",
    "    percentage_complex_words = len(complex_words) / len(cleaned_words) if len(cleaned_words) > 0 else 0\n",
    "\n",
    "    # Calculate Fog Index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Calculate Average Number of Words Per Sentence\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    # Calculate Complex Word Count\n",
    "    complex_word_count = len(complex_words)\n",
    "\n",
    "    # Calculate Word Count\n",
    "    word_count = len(cleaned_words)\n",
    "\n",
    "    # Calculate Syllable Per Word\n",
    "    syllable_per_word = sum(syllable_count(word) for word in cleaned_words) / len(cleaned_words) if len(cleaned_words) > 0 else 0\n",
    "\n",
    "    # Calculate Personal Pronouns Count\n",
    "    personal_pronouns_count = sum(1 for word in cleaned_words if word.lower() in {'i', 'we', 'my', 'ours', 'us'})\n",
    "\n",
    "    # Calculate Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in cleaned_words) / len(cleaned_words) if len(cleaned_words) > 0 else 0\n",
    "\n",
    "    # Return the computed variables\n",
    "    return {\n",
    "        'POSITIVE SCORE': positive_score,\n",
    "        'NEGATIVE SCORE': negative_score,\n",
    "        'POLARITY SCORE': polarity_score,\n",
    "        'SUBJECTIVITY SCORE': subjectivity_score,\n",
    "        'AVG SENTENCE LENGTH': avg_sentence_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
    "        'FOG INDEX': fog_index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': avg_words_per_sentence,\n",
    "        'COMPLEX WORD COUNT': complex_word_count,\n",
    "        'WORD COUNT': word_count,\n",
    "        'SYLLABLE PER WORD': syllable_per_word,\n",
    "        'PERSONAL PRONOUNS': personal_pronouns_count,\n",
    "        'AVG WORD LENGTH': avg_word_length\n",
    "    }\n",
    "\n",
    "def syllable_count(word):\n",
    "    # A simple function to count syllables in a word\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "\n",
    "    # Count consecutive vowels as one syllable\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            count += 1\n",
    "\n",
    "    # Adjust for silent 'e'\n",
    "    if word.endswith('e') and count > 1:\n",
    "        count -= 1\n",
    "\n",
    "    return max(count, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e94e72",
   "metadata": {},
   "source": [
    "#### finding  variables scores for the first 2 text files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3a9d519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables for blackassign0001.txt:\n",
      "{'POSITIVE SCORE': 30, 'NEGATIVE SCORE': 6, 'POLARITY SCORE': 0.6666666481481487, 'SUBJECTIVITY SCORE': 0.08017817354080585, 'AVG SENTENCE LENGTH': 15.5625, 'PERCENTAGE OF COMPLEX WORDS': 0.9933184855233853, 'FOG INDEX': 6.622327394209354, 'AVG NUMBER OF WORDS PER SENTENCE': 15.5625, 'COMPLEX WORD COUNT': 446, 'WORD COUNT': 449, 'SYLLABLE PER WORD': 2.4788418708240534, 'PERSONAL PRONOUNS': 0, 'AVG WORD LENGTH': 6.599109131403118}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Variables for blackassign0002.txt:\n",
      "{'POSITIVE SCORE': 48, 'NEGATIVE SCORE': 23, 'POLARITY SCORE': 0.35211267109700467, 'SUBJECTIVITY SCORE': 0.11993243222984387, 'AVG SENTENCE LENGTH': 18.085365853658537, 'PERCENTAGE OF COMPLEX WORDS': 0.9983108108108109, 'FOG INDEX': 7.63347066578774, 'AVG NUMBER OF WORDS PER SENTENCE': 18.085365853658537, 'COMPLEX WORD COUNT': 591, 'WORD COUNT': 592, 'SYLLABLE PER WORD': 2.8175675675675675, 'PERSONAL PRONOUNS': 0, 'AVG WORD LENGTH': 7.298986486486487}\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to read the content of a file\n",
    "def read_file_content(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# List of file names (adjust these based on your file names)\n",
    "file_names = [\"blackassign0001.txt\", \"blackassign0002.txt\"]\n",
    "\n",
    "# Loop through each file and analyze the text\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(os.getcwd(), file_name)\n",
    "\n",
    "    # Read the content of the file\n",
    "    text_content = read_file_content(file_path)\n",
    "\n",
    "    # Analyze the text and print the variables\n",
    "    variables = analyze_text(text_content)\n",
    "    print(f\"Variables for {file_name}:\")\n",
    "    print(variables)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b9e5e",
   "metadata": {},
   "source": [
    "#### finding scores  for all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b84c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to read the content of a file\n",
    "def read_file_content(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to extract URL from existing DataFrame based on URL_ID\n",
    "def extract_url(url_id, existing_df):\n",
    "    url_row = existing_df[existing_df['URL_ID'] == url_id]['URL']\n",
    "    return url_row.iloc[0] if not url_row.empty else ''\n",
    "\n",
    "# List all files in the current directory\n",
    "all_files = [file for file in os.listdir() if file.endswith(\".txt\")]\n",
    "\n",
    "# Create a list to store dictionaries for each file\n",
    "data = []\n",
    "\n",
    "# Read the existing Excel file\n",
    "existing_df = pd.read_excel(\"Output Data Structure.xlsx\")\n",
    "\n",
    "# Loop through each file and analyze the text\n",
    "for file_name in all_files:\n",
    "    file_path = os.path.join(os.getcwd(), file_name)\n",
    "\n",
    "    # Read the content of the file\n",
    "    text_content = read_file_content(file_path)\n",
    "\n",
    "    # Extract URL_ID from the file name\n",
    "    url_id = file_name.split('.')[0]\n",
    "\n",
    "    # Extract URL from the existing DataFrame\n",
    "    url = extract_url(url_id, existing_df)\n",
    "\n",
    "    # Analyze the text\n",
    "    variables = analyze_text(text_content)\n",
    "\n",
    "    # Create a dictionary for the current file\n",
    "    file_data = {'URL_ID': url_id, 'URL': url, **variables}\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    data.append(file_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f77cd511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.080178</td>\n",
       "      <td>15.562500</td>\n",
       "      <td>0.993318</td>\n",
       "      <td>6.622327</td>\n",
       "      <td>15.562500</td>\n",
       "      <td>446</td>\n",
       "      <td>449</td>\n",
       "      <td>2.478842</td>\n",
       "      <td>0</td>\n",
       "      <td>6.599109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>48</td>\n",
       "      <td>23</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.119932</td>\n",
       "      <td>18.085366</td>\n",
       "      <td>0.998311</td>\n",
       "      <td>7.633471</td>\n",
       "      <td>18.085366</td>\n",
       "      <td>591</td>\n",
       "      <td>592</td>\n",
       "      <td>2.817568</td>\n",
       "      <td>0</td>\n",
       "      <td>7.298986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>18.964912</td>\n",
       "      <td>0.995833</td>\n",
       "      <td>7.984298</td>\n",
       "      <td>18.964912</td>\n",
       "      <td>478</td>\n",
       "      <td>480</td>\n",
       "      <td>3.014583</td>\n",
       "      <td>0</td>\n",
       "      <td>8.089583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>29</td>\n",
       "      <td>62</td>\n",
       "      <td>-0.362637</td>\n",
       "      <td>0.195699</td>\n",
       "      <td>20.557692</td>\n",
       "      <td>0.997849</td>\n",
       "      <td>8.622217</td>\n",
       "      <td>20.557692</td>\n",
       "      <td>464</td>\n",
       "      <td>465</td>\n",
       "      <td>3.043011</td>\n",
       "      <td>0</td>\n",
       "      <td>8.051613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.085616</td>\n",
       "      <td>17.024390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.209756</td>\n",
       "      <td>17.024390</td>\n",
       "      <td>292</td>\n",
       "      <td>292</td>\n",
       "      <td>2.630137</td>\n",
       "      <td>0</td>\n",
       "      <td>7.383562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>-0.315789</td>\n",
       "      <td>0.156057</td>\n",
       "      <td>21.377358</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.950943</td>\n",
       "      <td>21.377358</td>\n",
       "      <td>487</td>\n",
       "      <td>487</td>\n",
       "      <td>2.704312</td>\n",
       "      <td>0</td>\n",
       "      <td>7.275154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.230769</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>27.325000</td>\n",
       "      <td>0.985465</td>\n",
       "      <td>11.324186</td>\n",
       "      <td>27.325000</td>\n",
       "      <td>339</td>\n",
       "      <td>344</td>\n",
       "      <td>2.502907</td>\n",
       "      <td>0</td>\n",
       "      <td>6.680233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>16.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.816000</td>\n",
       "      <td>16.040000</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>2.650538</td>\n",
       "      <td>0</td>\n",
       "      <td>7.198925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.071730</td>\n",
       "      <td>17.828571</td>\n",
       "      <td>0.966245</td>\n",
       "      <td>7.517926</td>\n",
       "      <td>17.828571</td>\n",
       "      <td>229</td>\n",
       "      <td>237</td>\n",
       "      <td>2.345992</td>\n",
       "      <td>0</td>\n",
       "      <td>6.417722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>27</td>\n",
       "      <td>45</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.987624</td>\n",
       "      <td>12.395050</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>399</td>\n",
       "      <td>404</td>\n",
       "      <td>2.690594</td>\n",
       "      <td>0</td>\n",
       "      <td>7.160891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0               30               6        0.666667            0.080178   \n",
       "1               48              23        0.352113            0.119932   \n",
       "2               34              20        0.259259            0.112500   \n",
       "3               29              62       -0.362637            0.195699   \n",
       "4               18               7        0.440000            0.085616   \n",
       "..             ...             ...             ...                 ...   \n",
       "95              26              50       -0.315789            0.156057   \n",
       "96              20              32       -0.230769            0.151163   \n",
       "97               5               3        0.250000            0.043011   \n",
       "98              15               2        0.764706            0.071730   \n",
       "99              27              45       -0.250000            0.178218   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0             15.562500                     0.993318   6.622327   \n",
       "1             18.085366                     0.998311   7.633471   \n",
       "2             18.964912                     0.995833   7.984298   \n",
       "3             20.557692                     0.997849   8.622217   \n",
       "4             17.024390                     1.000000   7.209756   \n",
       "..                  ...                          ...        ...   \n",
       "95            21.377358                     1.000000   8.950943   \n",
       "96            27.325000                     0.985465  11.324186   \n",
       "97            16.040000                     1.000000   6.816000   \n",
       "98            17.828571                     0.966245   7.517926   \n",
       "99            30.000000                     0.987624  12.395050   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                          15.562500                 446         449   \n",
       "1                          18.085366                 591         592   \n",
       "2                          18.964912                 478         480   \n",
       "3                          20.557692                 464         465   \n",
       "4                          17.024390                 292         292   \n",
       "..                               ...                 ...         ...   \n",
       "95                         21.377358                 487         487   \n",
       "96                         27.325000                 339         344   \n",
       "97                         16.040000                 186         186   \n",
       "98                         17.828571                 229         237   \n",
       "99                         30.000000                 399         404   \n",
       "\n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0            2.478842                  0         6.599109  \n",
       "1            2.817568                  0         7.298986  \n",
       "2            3.014583                  0         8.089583  \n",
       "3            3.043011                  0         8.051613  \n",
       "4            2.630137                  0         7.383562  \n",
       "..                ...                ...              ...  \n",
       "95           2.704312                  0         7.275154  \n",
       "96           2.502907                  0         6.680233  \n",
       "97           2.650538                  0         7.198925  \n",
       "98           2.345992                  0         6.417722  \n",
       "99           2.690594                  0         7.160891  \n",
       "\n",
       "[100 rows x 15 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e11db479",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_excel(\"Output Data Structure.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31375b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9489e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
